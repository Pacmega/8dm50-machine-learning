{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "In this practical session you will implement and train several Convolutional Neural Networks (CNNs) using the Keras framework with a Tensorflow backend. If you are not already familiar with Keras, you can go over the [following tutorial](https://github.com/tueimage/essential-skills/blob/master/keras.md). More detailed information on the different functionalities can be found in the [Keras library documentation](https://keras.io/). \n",
    "\n",
    "Note that for this set of exercise CPU-only Tensorflow, which you should already have installed, is sufficient (i.e. GPU-support is not required but it will make your experiments run faster). \n",
    "\n",
    "You are also required to use the `gryds` package for data augmentation that you can install directly from git: `pip install git+https://github.com/tueimage/gryds/`.\n",
    "\n",
    "You also have to install the Keras deep learning framework (if you have not done so already) by running `conda install keras`. Note that there are two implementations of Keras, one from https://keras.io/ and another one that ships with Tensorflow. Here we use the former. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "We will first train a simple CNN to classify handwritten digits using the MNIST dataset. This dataset is often referred to as the \"Hello world!\" example of deep learning because it can be used to quickly illustrate a small neural network in action (and obtain a decent classification accuracy in the process). More information on it can be found [here](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "First, let's load the dataset and visualize some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# load the MNIST the dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# scale the image intensities to the 0-1 range\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "# convert the data to channel-last\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# convert the labels to one-hot encoded\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "def plot_images(images, dim=(10, 10), figsize=(10, 10), title=''):\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(images[i], interpolation='nearest', cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_images(x_train[np.random.randint(0, x_train.shape[0], size=100)].reshape(100, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST classification task is quite simple: given an image, predict the digit that it contains. Thus, this is a 10-class classification problem.\n",
    "\n",
    "Let's define a simple network for the handwritten digit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and train the network (note that this could take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate its performance on the independent test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Conv2D(128, (12,12), activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Conv2D(10, (1,1), activation='softmax'))\n",
    "model2.add(Flatten())\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model2.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "model2.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score2[0])\n",
    "print('Test accuracy:', score2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net\n",
    "The U-Net convolutional neural network architecture was first developed for biomedical image segmentation and is to this day one of the most widely used methods for image segmentation. The details of the architecture can be found in the [original paper](https://arxiv.org/abs/1505.04597). In this practical we will build and train a U-Net network that is able to segment blood vessels in retinal images. \n",
    "\n",
    "### Loading and visualizing the data\n",
    "The data for this task is taken from the [DRIVE](https://www.isi.uu.nl/Research/Databases/DRIVE/index.php) database. It consists of photographs of the retina, where the goal is to segment the blood vessels within. The dataset has a total of 40 photographs, divided in 20 images for training and 20 for testing. The images corresponding to the DRIVE test set can be found [here](https://www.dropbox.com/s/zk51wgupimw7jd9/DRIVE.zip?dl=0).\n",
    "\n",
    "Let's load the training set and visualize an image with the corresponding blood vessel segmentation. For training we will divide the data in a training and a validation set to monitor the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('code/')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from unet_utils import load_data\n",
    "\n",
    "# location of the DRIVE dataset\n",
    "data_folder = 'C:/Users/stijn/Desktop/school/TUe/2022-2023/8dm50 Machine learning in medical imaging and biology/8dm50-machine-learning/practicals/data/DRIVE/'\n",
    "train_paths = glob(data_folder + 'training/images/*.tif')\n",
    "images, masks, segmentations = load_data(train_paths)\n",
    "\n",
    "# print the shape of image dataset\n",
    "print(images.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Image of the retina\")\n",
    "plt.axis('off')\n",
    "plt.imshow(images[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Ground truth vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(segmentations[0][:, :, 0])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# divide in training and validation\n",
    "train_images, val_images, train_masks, val_masks, train_segmentations, val_segmentations = train_test_split(\n",
    "    images, masks, segmentations, test_size=0.2, random_state=7)\n",
    "\n",
    "# print the shape of the training and valudation datasets\n",
    "print(train_images.shape)\n",
    "print(train_masks.shape)\n",
    "print(train_segmentations.shape)\n",
    "print(val_images.shape)\n",
    "print(val_masks.shape)\n",
    "print(val_segmentations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a U-Net model\n",
    "\n",
    "You are already provided with implementation of the U-Net architecture in `unet_model.py`. This is a modular implementation and can be used to generate U-Net architectures with a variety of hyperparameters such as depth and number of feature maps. Before using the model, examine the code and documentation and make sure that you understand all the details.\n",
    "\n",
    "We will train a U-Net model using smaller patches extracted from the training images. Training the images on smaller patches requires less computation power and results in a more varied training dataset (it has the effect of data augmentation by image translation). Because a U-Net is a fully convolutional network it can be evaluated on inputs of different size (the output size will change according to the input size). Thus, although the model will be trained on smaller patches it can still be used to segment larger images with one pass through the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_utils import extract_patches, preprocessing\n",
    "\n",
    "# work with 32x32 patches\n",
    "patch_size = (32, 32)\n",
    "\n",
    "# 200 patches per image\n",
    "patches_per_im = 200\n",
    "\n",
    "# visualize a couple of patches as a visual check\n",
    "patches, patches_segmentations = extract_patches(train_images, train_segmentations, patch_size, patches_per_im=1, seed=7)\n",
    "\n",
    "print(patches.shape)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 12))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 0].imshow(patches[i])\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 1].imshow(patches_segmentations[i][:, :, 0])\n",
    "    axes[i, 2].axis('off')\n",
    "    axes[i, 2].imshow(patches[i+5])\n",
    "    axes[i, 3].axis('off')\n",
    "    axes[i, 3].imshow(patches_segmentations[i+5][:, :, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the validation data to fit the U-Net model\n",
    "# images of shape (584, 565) shape result in concatenation error due to the odd number of columns\n",
    "\n",
    "print(\"Old shape:\", val_images.shape)\n",
    "\n",
    "val_images, val_masks, val_segmentations = preprocessing(\n",
    "    val_images, \n",
    "    val_masks, \n",
    "    val_segmentations, \n",
    "    desired_shape=(584, 584))\n",
    "    \n",
    "print(\"New shape:\", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from unet_model import unet\n",
    "from unet_utils import datagenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# use a single training image, to better demonstrate the effects of data augmentation\n",
    "X_train, y_train = np.expand_dims(train_images[0], axis=0), np.expand_dims(train_segmentations[0], axis=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# hyperparameters\n",
    "depth = 3\n",
    "channels = 32\n",
    "use_batchnorm = True\n",
    "batch_size = 64\n",
    "epochs = 250\n",
    "steps_per_epoch = int(np.ceil((patches_per_im * len(train_images)) / batch_size))\n",
    "\n",
    "# work with 32x32 patches\n",
    "patch_size = (32, 32)\n",
    "# 200 patches per image\n",
    "patches_per_im = 200\n",
    "\n",
    "# initialize model\n",
    "model = unet(input_shape=(None, None, 3), depth=depth, channels=channels, batchnorm=use_batchnorm)\n",
    "\n",
    "# print a summary of the model\n",
    "# model.summary(line_length=120)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# stop the training if the validation loss does not increase for 15 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# train the model with the data generator, and save the training history\n",
    "history = model.fit_generator(datagenerator(X_train, y_train, patch_size, patches_per_im, batch_size),\n",
    "                              validation_data=(val_images, val_segmentations),\n",
    "                              steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1,\n",
    "                              callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model on one test image and show the results\n",
    "from unet_utils import preprocessing\n",
    "\n",
    "# test data paths\n",
    "impaths_test = glob(data_folder + 'test/images/*.tif')\n",
    "\n",
    "# load data\n",
    "test_images, test_masks, test_segmentations = load_data(impaths_test, test=True)\n",
    "\n",
    "# pad the data to fit the U-Net model\n",
    "test_images, test_masks, test_segmentations = preprocessing(test_images, test_masks, test_segmentations, \n",
    "                                                            desired_shape=(584, 584))\n",
    "\n",
    "# use a single image to evaluate\n",
    "X_test, y_test = np.expand_dims(test_images[0], axis=0), np.expand_dims(test_masks[0], axis=0)\n",
    "\n",
    "# predict test samples\n",
    "test_prediction = model.predict(X_test, batch_size=4)\n",
    "\n",
    "# visualize the test result\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Image of the retina\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_images[0])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Ground truth vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_segmentations[0][:, :, 0])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_prediction[0, :, :, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Number of parameters\n",
    "\n",
    "The first convolutional layer in the MNIST example has 320 parameters. The first fully connected layer has 1179,776 parameters. What do these parameters correspond to? \n",
    "\n",
    "<font color='#770a0a'>What is the general expression for the number of parameters of 1) a convolutional layer and 2) a fully-connected layer?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer on the first question:**\n",
    "\n",
    "The 320 parameters in the first convolutional layer are $3*3=9$ weights per kernel plus a bias, meaning $10$ weights per kernel in total. With 32 kernels, this gives a total of 320 learnable weights.\n",
    "\n",
    "As for the first fully connected layer: the last conv layer had an output of $36,864$ values in a $24*24*64$ shape. Max pooling with a $2*2$ kernel and stride of $2$ then reduces this to $9,216$ in a $12*12*64$ shape. The flatten layer then changes the shape of this to $9,216*1*1$. In the FC layer connected to this each of the $128$ neurons then has a connection to each of these inputs plus a bias value. This gives a total number of learnable weights of $128*9,216 + 128 = 1,179,776$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General expressions\n",
    "**The Number of Parameters of a Conv Layer:**\n",
    "\n",
    "*$W_c$ = Number of weights of the Conv Layer.*\n",
    "\n",
    "*$B_c$ = Number of biases of the Conv Layer.*\n",
    "\n",
    "*$P_c$ = Number of parameters of the Conv Layer.*\n",
    "\n",
    "*$K$ = Size (width) of kernels used in the Conv Layer.*\n",
    "\n",
    "*$N$ = Number of kernels.*\n",
    "\n",
    "*$C$ = Number of channels of the input image.*\n",
    "\n",
    "\\begin{align}\n",
    "        W_c &= K^2 \\times C \\times N \\\\\n",
    "        B_c &= N \\\\\n",
    "        P_c &= W_c + B_c\n",
    "\\end{align}\n",
    "\n",
    "**Number of Parameters of a Fully Connected (FC) Layer connected to a Conv Layer:**\n",
    "\n",
    "*$W_{cf}$ = Number of weights of a FC Layer which is connected to a Conv Layer.*\n",
    "\n",
    "*$B_{cf}$ = Number of biases of a FC Layer which is connected to a Conv Layer.*\n",
    "\n",
    "*$P_{cf}$ = = Number of parameters of a FC Layer which is connected to a Conv Layer.*\n",
    "\n",
    "*$O$ = Size (width) of the output image of the **previous** Conv Layer.*\n",
    "\n",
    "*$N$ = Number of kernels in the previous Conv Layer.*\n",
    "\n",
    "*$F$ = Number of neurons in the FC Layer.*\n",
    "\n",
    "\\begin{align}\n",
    "        W_{cf} &= O^2 \\times N \\times F \\\\\n",
    "        B_{cf} &= F \\\\\n",
    "        P_{cf} &= W_{cf} + B_{cf}\n",
    "\\end{align}\n",
    "\n",
    "**The number of Parameters of a Fully Connected (FC) Layer connected to a FC Layer:**\n",
    "\n",
    "*$W_{ff}$ = Number of weights of a FC Layer which is connected to an FC Layer.*\n",
    "\n",
    "*$B_{ff}$ = Number of biases of a FC Layer which is connected to an FC Layer.*\n",
    "\n",
    "*$P_{ff}$ = Number of parameters of a FC Layer which is connected to an FC Layer.*\n",
    "\n",
    "*$F$ = Number of neurons in the FC Layer.*\n",
    "\n",
    "*$F_{-1}$ = Number of neurons in the previous FC Layer.*\n",
    "\n",
    "\\begin{align}\n",
    "        W_{ff} &= F_{-1} \\times F \\\\\n",
    "        B_{ff} &= F \\\\\n",
    "        P_{ff} &= W_{ff} + B_{ff}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-convolutional MNIST model\n",
    "\n",
    "Modify the model in the MNIST example in such a way that it only contains convolutional layers while keeping the same number of parameters. If you do the modification correctly, the two models will have the same behaviour (i.e. they will represent the same model, only with different implementation). Show this experimentally. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Conv2D(128, (12,12), activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Conv2D(10, (1,1), activation='softmax'))\n",
    "model2.add(Flatten())\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model2.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "model2.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=12,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score2[0])\n",
    "print('Test accuracy:', score2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer on the question:**\n",
    "\n",
    "The amount of parameters are exactly the same in every layer and the amount of total amount of trainable parameters is also the same. The training time and final accuracy and loss are also very close to eachother, which can be explained by a slight difference in initialized values while using the same model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## U-Net architecture\n",
    "\n",
    "<font color='#770a0a'> What is the role of the skip connections in the U-Net neural network architecture? Will it be possible to train the exact same architecture with the skip connections omitted? If yes, what would be the expected result? If no, what would be the cause of the error?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer on the question:**\n",
    "\n",
    "Yes this would be possible, however this would require the size of the model to increase, without any extra information being added. This would lead to a larger model, but a significant decrease in performance over the model with the skip connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "<font color='#770a0a'>Why does data augmentation result in less overfitting? Can data augmentation be applied to the test samples? If yes, towards what goal? If no, what is preventing that?</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer***\n",
    "\n",
    "With data augmentation the data size is increased. The model is less likely to overfit as it is unable to overfit on all the samples. Introducing slight variability in the data will therefore improve the generalizability of the model.  \n",
    "\n",
    "Data augmentation can be used on the training set to increase the number of samples and to increase the variability of our training data on wicht to fit the model on. The validation data can be augmented as well as it indicates when the model is most generalizable. Generalization happens only when in training the model and not during testing or validation and therefore data augmentation is not necessarily needed.\n",
    "\n",
    "The test set is used to score the model's real-world performance. Data augmentation in this case is not necessary as the model is trained on the augmented dataset which should represent variance in input that is encountered in the real-world samples. Therefore, data augmentation is not necessary on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part 2:*\n",
    "\n",
    "Implement random brightness augmentation of the image data by adding a random offset to the image intensity before passing them trough the network at training time. Train a model with random brightness augmentation and compare it to the baseline above. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from unet_utils import adjustbrightnessImages\n",
    "# Brightness augmentation is implemented in the function extract_patches_randBA\n",
    "# Patches are created for which their brightness is then augmented by a value delta, which is randomly chosen from the specified\n",
    "# range. Below an example is visualized.\n",
    "\n",
    "# work with 32x32 patches\n",
    "patch_size = (32, 32)\n",
    "\n",
    "# 200 patches per image\n",
    "patches_per_im = 200\n",
    "\n",
    "# range in which delta is chosen for brightness adjustment, which is added to all components of the image\n",
    "RBA_range = [-0.4,0.4]\n",
    "\n",
    "examp_patches = extract_patches_2d(train_images[0], patch_size, max_patches=patches_per_im, random_state=1)\n",
    "\n",
    "examp_patches_adj = adjustbrightnessImages(examp_patches,RBA_range)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 10))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 0].imshow(examp_patches[i])\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 1].imshow(examp_patches_adj[i])\n",
    "\n",
    "axes[0, 0].set_title('Original patch')\n",
    "axes[0, 1].set_title('Adjusted patch')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_utils import datagenerator_randBA, datagenerator_randBA_spline\n",
    "\n",
    "train_images, val_images, train_masks, val_masks, train_segmentations, val_segmentations = train_test_split(\n",
    "    images, masks, segmentations, test_size=0.2, random_state=7)\n",
    "\n",
    "\n",
    "val_images, val_masks, val_segmentations = preprocessing(\n",
    "    val_images, \n",
    "    val_masks, \n",
    "    val_segmentations, \n",
    "    desired_shape=(584, 584))\n",
    "\n",
    "# hyperparameters\n",
    "depth = 3\n",
    "channels = 32\n",
    "use_batchnorm = True\n",
    "batch_size = 64\n",
    "epochs = 250\n",
    "\n",
    "# work with 32x32 patches\n",
    "patch_size = (32, 32)\n",
    "\n",
    "# 200 patches per image\n",
    "patches_per_im = 200\n",
    "\n",
    "# range for random brightness augmentation\n",
    "brange = [-0.5,0.5]\n",
    "\n",
    "steps_per_epoch = int(np.ceil((patches_per_im * len(train_images)) / batch_size))\n",
    "\n",
    "\n",
    "# initialize model\n",
    "model = unet(input_shape=(None, None, 3), depth=depth, channels=channels, batchnorm=use_batchnorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# stop the training if the validation loss does not increase for 15 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "\n",
    "history = model.fit_generator(datagenerator_randBA(train_images, train_segmentations, patch_size, patches_per_im, brange, batch_size),\n",
    "                              validation_data=(val_images, val_segmentations),\n",
    "                              steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1,\n",
    "                              callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = model.evaluate(test_images, test_segmentations, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a single image to evaluate\n",
    "X_test, y_test = np.expand_dims(test_images[0], axis=0), np.expand_dims(test_masks[0], axis=0)\n",
    "\n",
    "# predict test samples\n",
    "test_prediction = model.predict(X_test, batch_size=4)\n",
    "\n",
    "# visualize the test result\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Image of the retina\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_images[0])\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Ground truth vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_segmentations[0][:, :, 0])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted vessel segmentation\")\n",
    "plt.axis('off')\n",
    "plt.imshow(test_prediction[0, :, :, 0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part 3:*\n",
    "\n",
    "Implement data augmentation procedure that in addition to brightness augmentation also performs b-spline geometric augmentation using the [`gryds`](https://github.com/tueimage/gryds) package (you can look at the documentation of the package for an example on how to do that). Compare the new model with the baseline and the model that only performs brightness augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from unet_utils import bsplinetransform\n",
    "\n",
    "# B-spline geometric augmentation is implemented in the function extract_patches_randBA_spline\n",
    "# Patches are created for which their brightness is then augmented by a value delta, which is randomly chosen from the specified\n",
    "# range. The image is transformed with the function bsplinetransform. Below an example is visualized.\n",
    "\n",
    "# work with 32x32 patches\n",
    "patch_size = (32, 32)\n",
    "\n",
    "# 200 patches per image\n",
    "patches_per_im = 200\n",
    "\n",
    "# range in which delta is chosen for brightness adjustment, which is added to all components of the image\n",
    "RBA_range = [-0.4,0.4]\n",
    "bsplinerange = [-0.05,0.05]\n",
    "\n",
    "examp_patches_im = extract_patches_2d(train_images[0], patch_size, max_patches=patches_per_im, random_state=1)\n",
    "examp_patches_seg = np.expand_dims(extract_patches_2d(train_segmentations[0], patch_size, max_patches=patches_per_im, random_state=1),\n",
    "                                   axis=-1)\n",
    "\n",
    "# bspline does\n",
    "examp_im_trans, examp_seg_trans = bsplinetransform(examp_patches_im, examp_patches_seg, bounds=bsplinerange)\n",
    "\n",
    "examp_im_adj = adjustbrightnessImages(examp_im_trans,RBA_range)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(17, 17))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 0].imshow(examp_patches_im[i])\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 1].imshow(examp_im_trans[i])\n",
    "    axes[i, 2].axis('off')\n",
    "    axes[i, 2].imshow(examp_im_adj[i])\n",
    "    axes[i, 3].axis('off')\n",
    "    axes[i, 3].imshow(examp_patches_seg[i])\n",
    "    axes[i, 4].axis('off')\n",
    "    axes[i, 4].imshow(examp_seg_trans[i])\n",
    "    \n",
    "axes[0, 0].set_title('Original patch')\n",
    "axes[0, 1].set_title('Adjusted patch: b-spline')\n",
    "axes[0, 2].set_title('Adjusted patch: brightness')\n",
    "axes[0, 3].set_title('Original segmentation')\n",
    "axes[0, 4].set_title('Adjusted segmentation: b-spline')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splinebounds = [-0.05,0.05]\n",
    "\n",
    "model_3 = unet(input_shape=(None, None, 3), depth=depth, channels=channels, batchnorm=use_batchnorm)\n",
    "\n",
    "model_3.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stop the training if the validation loss does not increase for 15 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# train the model with the data generator, and save the training history\n",
    "history_RBA_spline = model_3.fit_generator(datagenerator_randBA_spline(train_images, train_segmentations, patch_size, patches_per_im, brange, splinebounds, batch_size),\n",
    "                              validation_data=(val_images, val_segmentations),\n",
    "                              steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1,\n",
    "                              callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_3.evaluate(test_images, test_segmentations, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
